# -*- coding: utf-8 -*-
"""addiction_analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jxZf1krzhBhbmElWpayfXH_7mQ7mPZ8f
"""

# Social Media & Smartphones Addiction Analysis Project
# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from xgboost import XGBRegressor, XGBClassifier
from sklearn.svm import SVR
from sklearn.metrics import (mean_squared_error, r2_score,
                           accuracy_score, confusion_matrix,
                           classification_report, roc_auc_score,
                           roc_curve, precision_recall_curve)
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import dash
from dash import dcc, html
from dash.dependencies import Input, Output
import warnings
warnings.filterwarnings('ignore')

# Initialize Dash app
app = dash.Dash(__name__)

# Load dataset
df = pd.read_csv('mobile_usage_behavioral_analysis.csv')

# Data preprocessing
def preprocess_data(df):
    # Handle missing values
    df = df.dropna()

    # Convert categorical variables
    le = LabelEncoder()
    df['Gender'] = le.fit_transform(df['Gender'])

    # Feature engineering - create addiction score based on usage patterns
    df['Addiction_Score'] = (df['Total_App_Usage_Hours'] * 0.4 +
                            df['Daily_Screen_Time_Hours'] * 0.3 +
                            df['Social_Media_Usage_Hours'] * 0.3)

    # Create addiction status (binary classification target)
    df['Addiction_Status'] = np.where(df['Addiction_Score'] > df['Addiction_Score'].quantile(0.75), 1, 0)

    return df

df = preprocess_data(df)

# Split data for regression (predict Addiction_Score)
X_reg = df.drop(['Addiction_Score', 'Addiction_Status', 'User_ID', 'Location'], axis=1)
y_reg = df['Addiction_Score']
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(
    X_reg, y_reg, test_size=0.2, random_state=42)

# Split data for classification (predict Addiction_Status)
X_clf = df.drop(['Addiction_Status', 'Addiction_Score', 'User_ID', 'Location'], axis=1)
y_clf = df['Addiction_Status']
X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(
    X_clf, y_clf, test_size=0.2, random_state=42)

# Scale features
scaler = StandardScaler()
X_train_reg_scaled = scaler.fit_transform(X_train_reg)
X_test_reg_scaled = scaler.transform(X_test_reg)
X_train_clf_scaled = scaler.fit_transform(X_train_clf)
X_test_clf_scaled = scaler.transform(X_test_clf)

# Static visualizations
def create_static_visualizations(df):
    plt.figure(figsize=(18, 12))

    # Distribution of addiction score
    plt.subplot(2, 3, 1)
    sns.histplot(df['Addiction_Score'], kde=True, bins=20)
    plt.title('Distribution of Addiction Scores')

    # Usage by gender
    plt.subplot(2, 3, 2)
    sns.boxplot(x='Gender', y='Daily_Screen_Time_Hours', data=df)
    plt.title('Daily Screen Time by Gender')
    plt.xticks([0, 1], ['Male', 'Female'])

    # Correlation heatmap
    plt.subplot(2, 3, 3)
    corr = df.corr()
    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.1f')
    plt.title('Correlation Heatmap')

    # Addiction status count
    plt.subplot(2, 3, 4)
    sns.countplot(x='Addiction_Status', data=df)
    plt.title('Addiction Status Distribution')
    plt.xticks([0, 1], ['Normal', 'Addicted'])

    # Social media vs productivity usage
    plt.subplot(2, 3, 5)
    sns.scatterplot(x='Social_Media_Usage_Hours', y='Productivity_App_Usage_Hours',
                   hue='Addiction_Status', data=df)
    plt.title('Social Media vs Productivity App Usage')

    # Age vs Addiction Score
    plt.subplot(2, 3, 6)
    sns.regplot(x='Age', y='Addiction_Score', data=df, scatter_kws={'alpha':0.3})
    plt.title('Age vs Addiction Score')

    plt.tight_layout()
    plt.savefig('static_visualizations.png')
    plt.show()

create_static_visualizations(df)

# Interactive visualizations with Plotly
def create_interactive_visualizations(df):
    # Interactive scatter plot
    fig1 = px.scatter(df, x='Daily_Screen_Time_Hours', y='Addiction_Score',
                     color='Gender', size='Number_of_Apps_Used',
                     hover_data=['Age', 'Productivity_App_Usage_Hours'],
                     title='Screen Time vs Addiction Score by Gender')
    fig1.write_html("interactive_scatter.html")

    # Interactive box plot of usage by addiction status
    fig2 = px.box(df, x='Addiction_Status', y='Total_App_Usage_Hours',
                 color='Gender', points="all",
                 title='Total App Usage by Addiction Status and Gender')
    fig2.write_html("interactive_box.html")

    # Interactive 3D scatter
    fig3 = px.scatter_3d(df, x='Daily_Screen_Time_Hours',
                        y='Social_Media_Usage_Hours',
                        z='Addiction_Score',
                        color='Addiction_Status',
                        title='3D Relationship: Screen Time, Social Media, and Addiction')
    fig3.write_html("interactive_3d.html")

    return fig1, fig2, fig3

fig1, fig2, fig3 = create_interactive_visualizations(df)

# Regression analysis
def run_regression_analysis(X_train, X_test, y_train, y_test):
    results = {}

    # Linear Regression
    lr = LinearRegression()
    lr.fit(X_train, y_train)
    y_pred_lr = lr.predict(X_test)
    results['Linear Regression'] = {
        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_lr)),
        'R2': r2_score(y_test, y_pred_lr),
        'model': lr
    }

    # Random Forest Regression
    rf = RandomForestRegressor(n_estimators=100, random_state=42)
    rf.fit(X_train, y_train)
    y_pred_rf = rf.predict(X_test)
    results['Random Forest'] = {
        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_rf)),
        'R2': r2_score(y_test, y_pred_rf),
        'model': rf
    }

    # XGBoost Regression (Bonus)
    xgb = XGBRegressor(n_estimators=100, random_state=42)
    xgb.fit(X_train, y_train)
    y_pred_xgb = xgb.predict(X_test)
    results['XGBoost'] = {
        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_xgb)),
        'R2': r2_score(y_test, y_pred_xgb),
        'model': xgb
    }

    # SVR (Bonus)
    svr = SVR(kernel='rbf')
    svr.fit(X_train, y_train)
    y_pred_svr = svr.predict(X_test)
    results['SVR'] = {
        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_svr)),
        'R2': r2_score(y_test, y_pred_svr),
        'model': svr
    }

    # Plot results
    plt.figure(figsize=(12, 8))
    for model_name, metrics in results.items():
        plt.scatter(y_test, metrics['model'].predict(X_test),
                   alpha=0.3,
                   label=f"{model_name} (R2={metrics['R2']:.2f})")
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')
    plt.xlabel('Actual Addiction Score')
    plt.ylabel('Predicted Addiction Score')
    plt.title('Regression Model Comparison')
    plt.legend()
    plt.savefig('regression_results.png')
    plt.show()

    return results

reg_results = run_regression_analysis(X_train_reg_scaled, X_test_reg_scaled,
                                    y_train_reg, y_test_reg)

# Classification analysis
def run_classification_analysis(X_train, X_test, y_train, y_test):
    results = {}

    # Logistic Regression
    lr = LogisticRegression(max_iter=1000, random_state=42)
    lr.fit(X_train, y_train)
    y_pred_lr = lr.predict(X_test)
    y_proba_lr = lr.predict_proba(X_test)[:, 1]
    results['Logistic Regression'] = {
        'accuracy': accuracy_score(y_test, y_pred_lr),
        'report': classification_report(y_test, y_pred_lr),
        'roc_auc': roc_auc_score(y_test, y_proba_lr),
        'model': lr
    }

    # Random Forest Classifier
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(X_train, y_train)
    y_pred_rf = rf.predict(X_test)
    y_proba_rf = rf.predict_proba(X_test)[:, 1]
    results['Random Forest'] = {
        'accuracy': accuracy_score(y_test, y_pred_rf),
        'report': classification_report(y_test, y_pred_rf),
        'roc_auc': roc_auc_score(y_test, y_proba_rf),
        'model': rf
    }

    # XGBoost Classifier
    xgb = XGBClassifier(n_estimators=100, random_state=42)
    xgb.fit(X_train, y_train)
    y_pred_xgb = xgb.predict(X_test)
    y_proba_xgb = xgb.predict_proba(X_test)[:, 1]
    results['XGBoost'] = {
        'accuracy': accuracy_score(y_test, y_pred_xgb),
        'report': classification_report(y_test, y_pred_xgb),
        'roc_auc': roc_auc_score(y_test, y_proba_xgb),
        'model': xgb
    }

    # Neural Network (Bonus)
    model = keras.Sequential([
        layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
        layers.Dropout(0.2),
        layers.Dense(32, activation='relu'),
        layers.Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam',
                loss='binary_crossentropy',
                metrics=['accuracy'])
    model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)
    y_pred_nn = (model.predict(X_test) > 0.5).astype(int)
    y_proba_nn = model.predict(X_test)
    results['Neural Network'] = {
        'accuracy': accuracy_score(y_test, y_pred_nn),
        'report': classification_report(y_test, y_pred_nn),
        'roc_auc': roc_auc_score(y_test, y_proba_nn),
        'model': model
    }

    # Plot ROC curves
    plt.figure(figsize=(10, 8))
    for model_name, metrics in results.items():
        if model_name != 'Neural Network':
            fpr, tpr, _ = roc_curve(y_test, metrics['model'].predict_proba(X_test)[:, 1])
        else:
            fpr, tpr, _ = roc_curve(y_test, metrics['model'].predict(X_test).ravel())
        plt.plot(fpr, tpr, label=f"{model_name} (AUC={metrics['roc_auc']:.2f})")
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve Comparison')
    plt.legend()
    plt.savefig('classification_roc.png')
    plt.show()

    return results

clf_results = run_classification_analysis(X_train_clf_scaled, X_test_clf_scaled,
                                        y_train_clf, y_test_clf)

# Dashboard layout
app.layout = html.Div([
    html.H1("Smartphone & Social Media Addiction Analysis Dashboard"),

    dcc.Tabs([
        dcc.Tab(label='Data Overview', children=[
            html.H3("Dataset Overview"),
            dcc.Graph(figure=fig1),
            html.Div([
                html.H4("Key Statistics"),
                html.Pre(df.describe().to_string())
            ])
        ]),

        dcc.Tab(label='Regression Analysis', children=[
            html.H3("Regression Model Performance"),
            dcc.Dropdown(
                id='reg-model-selector',
                options=[{'label': model, 'value': model}
                        for model in reg_results.keys()],
                value='Linear Regression'
            ),
            dcc.Graph(id='reg-results-plot'),
            html.Div([
                html.H4("Model Metrics"),
                html.Pre(id='reg-metrics')
            ])
        ]),

        dcc.Tab(label='Classification Analysis', children=[
            html.H3("Classification Model Performance"),
            dcc.Dropdown(
                id='clf-model-selector',
                options=[{'label': model, 'value': model}
                        for model in clf_results.keys()],
                value='Logistic Regression'
            ),
            dcc.Graph(id='clf-results-plot'),
            html.Div([
                html.H4("Model Metrics"),
                html.Pre(id='clf-metrics')
            ])
        ]),

        dcc.Tab(label='Interactive Visualizations', children=[
            html.H3("Interactive Data Exploration"),
            dcc.Graph(figure=fig2),
            dcc.Graph(figure=fig3)
        ])
    ])
])

# Callbacks for dashboard interactivity
@app.callback(
    [Output('reg-results-plot', 'figure'),
     Output('reg-metrics', 'children')],
    [Input('reg-model-selector', 'value')]
)
def update_regression_results(selected_model):
    model = reg_results[selected_model]['model']
    y_pred = model.predict(X_test_reg_scaled)

    # Create actual vs predicted plot
    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=y_test_reg,
        y=y_pred,
        mode='markers',
        name='Predictions'
    ))
    fig.add_trace(go.Scatter(
        x=[y_test_reg.min(), y_test_reg.max()],
        y=[y_test_reg.min(), y_test_reg.max()],
        mode='lines',
        name='Perfect Prediction',
        line=dict(color='black', dash='dash')
    ))
    fig.update_layout(
        title=f'{selected_model} - Actual vs Predicted Addiction Scores',
        xaxis_title='Actual Scores',
        yaxis_title='Predicted Scores'
    )

    # Create metrics text
    metrics_text = f"""
    Model: {selected_model}
    RMSE: {reg_results[selected_model]['RMSE']:.2f}
    R-squared: {reg_results[selected_model]['R2']:.2f}
    """

    return fig, metrics_text

@app.callback(
    [Output('clf-results-plot', 'figure'),
     Output('clf-metrics', 'children')],
    [Input('clf-model-selector', 'value')]
)
def update_classification_results(selected_model):
    model = clf_results[selected_model]['model']

    if selected_model != 'Neural Network':
        y_proba = model.predict_proba(X_test_clf_scaled)[:, 1]
    else:
        y_proba = model.predict(X_test_clf_scaled).ravel()

    # Create ROC curve
    fpr, tpr, _ = roc_curve(y_test_clf, y_proba)
    roc_auc = roc_auc_score(y_test_clf, y_proba)

    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=fpr,
        y=tpr,
        mode='lines',
        name=f'ROC curve (AUC = {roc_auc:.2f})'
    ))
    fig.add_trace(go.Scatter(
        x=[0, 1],
        y=[0, 1],
        mode='lines',
        name='Random',
        line=dict(color='black', dash='dash')
    ))
    fig.update_layout(
        title=f'{selected_model} - ROC Curve',
        xaxis_title='False Positive Rate',
        yaxis_title='True Positive Rate'
    )

    # Create metrics text
    metrics_text = f"""
    Model: {selected_model}
    Accuracy: {clf_results[selected_model]['accuracy']:.2f}
    AUC: {clf_results[selected_model]['roc_auc']:.2f}

    Classification Report:
    {clf_results[selected_model]['report']}
    """

    return fig, metrics_text

# Run the dashboard
if __name__ == '__main__':
    app.run_server(debug=True)